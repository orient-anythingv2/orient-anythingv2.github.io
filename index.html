<!DOCTYPE html>
<html>

<head>
    <meta name="viewport" content="width=device-width, initial-scale=0.2">
    <meta charset="utf-8">
    <meta name="description" content="Orient Anything">
    <meta name="keywords" content="Monocular Orientation Estimation">
    <title>Orient Anything</title>

    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/Logo.png">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="./static/js/script.js"></script>
</head>

<body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
    </nav>

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">
                            [NeurIPS 2025 Spotlight]<br>
                            Orient Anything V2: Unifying Orientation and Rotation Understanding
                        </h1>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=euXK0lkAAAAJ">Zehan
                                    Wang</a><sup>1*</sup>
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?hl=zh-CN&user=DptGMnYAAAAJ">Ziang
                                    Zhang</a><sup>1*</sup>
                            </span>
                            <span class="author-block">
                                <a href="https://github.com/1339354001">Jiayang Xu</a><sup>1</sup>
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?hl=en&user=OIuFz1gAAAAJ">Jialei
                                    Wang</a><sup>1</sup>
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?hl=zh-CN&user=wYDbtFsAAAAJ">Tianyu
                                    Pang</a><sup>2</sup>
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?hl=zh-CN&user=QOp7xW0AAAAJ">Chao 
                                    Du</a><sup>2</sup>
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=4uE10I0AAAAJ&hl&oi=ao">Hengshuang
                                    Zhao</a><sup>3</sup>
                            </span>
                            <span class="author-block">
                                <a href="https://scholar.google.com/citations?user=IIoFY90AAAAJ&hl&oi=ao">Zhou
                                    Zhao</a><sup>1</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Zhejiang University</span>&emsp;&emsp;&emsp;&emsp;
                            <span class="author-block"><sup>2</sup>SEA AI Lab</span>&emsp;&emsp;&emsp;&emsp;
                            <span class="author-block"><sup>3</sup>HKU</span>
                        </div>

                        <p class="is-size-6"><em>*Equal Contribution</em></p>

                        <div class="publication-links" style="margin-top: 1rem;">
                            <a href="https://openreview.net/pdf?id=n3armuTFit" target="_blank"
                                class="button is-normal is-rounded"
                                style="background-color: #ff4d4d; color: white; margin: 0.25rem;">
                                <span class="icon"><i class="ai ai-arxiv"></i></span>
                                <span>arXiv PDF</span>
                            </a>
                            <a href="https://orient-anythingv2.github.io" target="_blank"
                                class="button is-normal is-rounded"
                                style="background-color: #4ade80; color: black; margin: 0.25rem;">
                                <span class="icon"><i class="fas fa-home"></i></span>
                                <span>Project Page</span>
                            </a>
                            <a href="https://huggingface.co/spaces/Viglong/Orient-Anything-V2" target="_blank"
                                class="button is-normal is-rounded"
                                style="background-color: #4361ee; color: white; margin: 0.25rem;">
                                <span class="icon"><i class="fas fa-laptop"></i></span>
                                <span>Hugging Face Demo</span>
                            </a>
                            <a href="https://huggingface.co/datasets/Viglong/OriAnyV2_Train_Render" target="_blank"
                                class="button is-normal is-rounded"
                                style="background-color: #f77f00; color: white; margin: 0.25rem;">
                                <span class="icon"><i class="fas fa-database"></i></span>
                                <span>Train Data</span>
                            </a>
                            <a href="https://huggingface.co/datasets/Viglong/OriAnyV2_Inference" target="_blank"
                                class="button is-normal is-rounded"
                                style="background-color: #f77f00; color: white; margin: 0.25rem;">
                                <span class="icon"><i class="fas fa-database"></i></span>
                                <span>Test Data</span>
                            </a>
                            <a href="https://huggingface.co/papers/2412.18605" target="_blank"
                                class="button is-normal is-rounded"
                                style="background-color: #ffba08; color: black; margin: 0.25rem;">
                                <span class="icon"><i class="fas fa-book"></i></span>
                                <span>Hugging Face Paper</span>
                            </a>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <b>Note:</b> To avoid ambiguity, Orient Anything V2 only accept images containing a single object as input. For
            multi-object scenarios, we first use SAM to isolate each object and then predict their orientation
            separately.
            <div class="hero-body">
                <img id="demo" width="100%" src="static/images/figures/overview.jpg" />
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <h4 class="subtitle has-text-centered">
                    <div class="content has-text-justified">
                        <p>
                            <strong>Orient Anything V2</strong>, a unified spatial vision model for understanding
                            orientation, symmetry, and relative rotation, achieves SOTA performance across 14 datasets. For
                            visualization, the object orientation is represented by the red axis,
                            while the blue and green axes indicate the upward and left sides of the object.
                        </p>
                    </div>
                </h4>
            </div>
        </div>
    </section>

    <section class="hero is-small" style="margin-top: 30px; padding-top: 0px;">
        <h2 class="title is-4" style="text-align: center;">Visualizations on Images in the wild.</h2>
        <div class="container is-max-desktop">
            <div style="text-align: center; border: green solid 0px;">
                <img alt="" id="wild1" width="70%" src="static/images/demos/no0.jpg"
                    style="display: inline-block; vertical-align: middle;" />
                <img alt="" id="wild2" width="70%" src="static/images/demos/one0.jpg" style="margin: 0 auto" />
                <img alt="" id="wild2" width="70%" src="static/images/demos/one1.jpg" style="margin: 0 auto" />
                <img alt="" id="wild2" width="70%" src="static/images/demos/two0.jpg" style="margin: 0 auto" />
                <img alt="" id="wild2" width="70%" src="static/images/demos/four0.jpg" style="margin: 0 auto" />
            </div>
        </div>
    </section>

    <section class="hero is-small" style="margin-top: 30px; padding-top: 0px;">
        <h2 class="title is-4" style="text-align: center;">Visualization of relative pose prediction.</h2>
        <div class="container is-max-desktop">
            <div style="text-align: center; border: green solid 0px;">
                <img alt="" id="wild1" width="70%" src="static/images/demos/ref0.jpg"
                    style="display: inline-block; vertical-align: middle;" />
                <img alt="" id="wild2" width="70%" src="static/images/demos/ref1.jpg" style="margin: 0 auto" />
            </div>
        </div>
    </section>

    <!-- <section class="section" style="background-color:#efeff081"> -->
    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-4">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            This work presents <strong>Orient Anything V2</strong>, an enhanced foundation model for unified understanding of 
                            <span style="color: #cc6511;">object 3D orientation and rotation</span> from 
                            <span style="color: #2271cc;">single or paired images</span>. 
                            Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. 
                            These improvements are enabled by four key innovations: 
                            <strong>1)</strong> Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 
                            <strong>2)</strong> An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 
                            <strong>3)</strong> A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 
                            <strong>4)</strong> A multi-frame architecture that directly predicts relative object rotations. 
                            Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on 
                            <em>orientation estimation</em>, <em>6DoF pose estimation</em>, and <em>object symmetry recognition</em> across 11 widely used benchmarks. 
                            The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-4" style="text-align: center;">3D Asset Synthesis</h2>
                    <div class="content has-text-justified">
                        <p>
                            The synthetic 3D asset generation pipeline is composed of three steps: <strong>1)</strong> Class Tag → Caption: 
                            Starting from ImageNet-21K category tags, we use Qwen-2.5 to generate rich, descriptive captions 
                            that capture object attributes and pose variations, ensuring broad and diverse category coverage.
                            <strong>2)</strong> Caption → Image: Leveraging the FLUX.1-Dev text-to-image model, we synthesize high-fidelity 
                            images from captions, enhanced with positional descriptors to encourage upright poses and explicit 
                            3D structure. <strong>3)</strong> Image → 3D Mesh: Using Hunyuan-3D-2.0, we convert the generated images into high-quality 
                            3D meshes with complete geometry and detailed textures. 
                        </p>
                        
                    </div>
                    <div class="image-area">
                        <img src="static/images/figures/generation.jpg" alt="generation" width="100%">
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-4" style="text-align: center;">Robust Annotation</h2>
                    <div class="content has-text-justified">
                        <p>
                            The annotation pipeline features two stages. First, an enhanced Orient-Anything-V1 model 
                            generates pseudo-labels from multiple rendered views of each 3D asset; these are projected 
                            into a shared 3D space and aggregated to infer dominant front-facing directions and rotational 
                            symmetry (e.g., single-front, bilateral, or full symmetry), effectively suppressing view-specific errors.
                        </p>
                        <p>
                            Second, a human-in-the-loop consistency check is applied at the category level: 
                            assets within the same ImageNet-21K class are expected to share the same symmetry type. 
                            Categories with consistent annotations are auto-accepted; Categories with consistent annotations are auto-accepted.
                            Inconsistent ones, which account for only about 15% of the 21k classes and usually involve few assets, are flagged for manual review.
                        </p>
                    </div>
                    <div class="image-area">
                        <img src="static/images/figures/annotation.jpg" alt="annotaion" width="100%">
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="columns is-centered">
                    <div class="column is-four-fifths">
                        <h2 class="title is-4" style="text-align: center;">Model Training</h2>
                        <div class="content has-text-justified">
                            <p>
                                Orient-Anything-V2 builds on the VGGT backbone and employs multi-task heads for prediction.
                                The model is trained to estimate the 3D distribution of absolute pose and symmetry for the 
                                first-frame image, as well as the 3D distribution of relative rotation between the second 
                                frame and the first.
                            </p>
                        </div>

                        <div class="image-area">
                            <img src="static/images/figures/model.jpg" alt="model" width="100%" class="center">
                        </div>
    </section>


    <section class="section pt-0" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title is-4">Citation</h2>
            <pre class="selectable"><code>@inproceedings{wangorient,
title={Orient Anything V2: Unifying Orientation and Rotation Understanding},
author={Wang, Zehan and Zhang, Ziang and Xu, Jiayang and Wang, Jialei and Pang, Tianyu and Du, Chao and Zhao, Hengshuang and Zhao, Zhou},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems}
}</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="https://openreview.net/pdf?id=n3armuTFit" class="external-link"
                    disabled>
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/SpatialVision/Orient-Anything-V2" class="external-link"
                    disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="content">
                    <p>
                        The template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
                    </p>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>